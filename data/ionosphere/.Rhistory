c = n/(n-1)/(n-1)
invCOV = solve(COV)
xn = t(as.matrix(X[1000,1:2]))
(n-2)/(n-1)*(invCOV) + c*(n-2)/(n-1)*invCOV%*%xn%*%t(xn)%*%invCOV/as.numeric((1-c*t(xn)%*%invCOV%*%xn))
COVd = cov(X[1:999,1:2])
invCOVd = solve(COVd)
invCOVd
library(MASS)
X = synth.te
library(MASS)
X = synth.te[,1:2]
cov(X)
X[,1] = (X[,1] - colMeans(X)[1])
X[,2] = (X[,2] - colMeans(X)[2])
colMeans(X)
cov(X)
n = nrow(X)
COV = cov(X)
c = n/(n-1)/(n-1)
invCOV = solve(COV)
xn = t(as.matrix(X[1000,1:2]))
(n-2)/(n-1)*(invCOV) + c*(n-2)/(n-1)*invCOV%*%xn%*%t(xn)%*%invCOV/as.numeric((1-c*t(xn)%*%invCOV%*%xn))
COVd = cov(X[1:999,1:2])
invCOVd = solve(COVd)
invCOVd
system.time(for(i in 1:10000){(n-2)/(n-1)*(invCOV) + c*(n-2)/(n-1)*invCOV%*%xn%*%t(xn)%*%invCOV/as.numeric((1-c*t(xn)%*%invCOV%*%xn))
})
system.time(for(i in 1:10000){COVd = cov(X[1:999,1:2])
invCOVd = solve(COVd)})
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
table(data$V9)
names(data)[9] = 'outcome'
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
table(data$V9)
names(data)[9] = 'y'
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
model = glm(y~., data = train, family=binomial(link='logit'))
result = predict(model, newdata = test)
library(PRROC)
library(MLmetrics)
pr <- pr.curve(scores.class0 = na.omit(result[test$y == 1]),
scores.class1 = na.omit(result[test$y == 0]), curve = T)
pr
pr$auc.integral
library(zoo)
dat<-data.frame(x = pr$curve[,1],y = pr$curve[,2])
dat
t1 = tapply(dat$y, dat$x, max)
t2 = tapply(dat$y, dat$x, min)
t = c(t1,t2)
library(zoo)
x <- as.numeric(as.character(names(t)))
y <- as.numeric(t)
id <- order(x)
auc_3[1,3]<-
auc_3[1,3]<-
sum(diff(x[id])*rollmean(y[id],2))
um(diff(x[id])*rollmean(y[id],2))
sum(diff(x[id])*rollmean(y[id],2))
pr_table = as.data.frame(pr$curve)
p0 = sum(test$y)/nrow(test)
pr_table$alarm_rate = (pr_table[,2]/pr_table[,1])*p0
maxless <- max(pr_table$alarm_rate[pr_table$alarm_rate <= 0.005])
recall = pr_table[which(pr_table$alarm_rate == maxless),2]
auc_3[1,4] = recall[1]
recall[1]
pr_table
maxless <- max(pr_table$alarm_rate[pr_table$alarm_rate <= 0.3])
recall = pr_table[which(pr_table$alarm_rate == maxless),2]
recall
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
table(data$V9)
names(data)[9] = 'y'
#table(data$outcome)
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
model = glm(y~., data = train, family=binomial(link='logit'))
result = predict(model, newdata = test)
library(PRROC)
library(MLmetrics)
pr <- pr.curve(scores.class0 = na.omit(result[test$y == 1]),
scores.class1 = na.omit(result[test$y == 0]), curve = T)
pr_table = as.data.frame(pr$curve)
p0 = sum(test$y)/nrow(test)
pr_table$alarm_rate = (pr_table[,2]/pr_table[,1])*p0
ar = 0.3
maxless <- max(pr_table$alarm_rate[pr_table$alarm_rate <= ar])
recall = pr_table[which(pr_table$alarm_rate == maxless),2]
recall
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
table(data$V9)
names(data)[9] = 'outcome'
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
table(data$outcome)
View(train)
gm<-mvnormalmixEM(train[,-9], k=2)
library(mixtools)
gm<-mvnormalmixEM(train[,-9], k=2)
gm
label = apply(gm$posterior, 1, which.max)
label
table(label)
default = train[train$outcome == 1,]
gm<-mvnormalmixEM(default[,-9], k=2)
label = apply(gm$posterior, 1, which.max)
table(label)
default$outcome = label
relabel = rbind(train[train$yc == 0,],default)
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
table(data$V9)
names(data)[9] = 'outcome'
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
table(data$outcome)
library(mixtools)
default = train[train$outcome == 1,]
gm<-mvnormalmixEM(default[,-9], k=2)
label = apply(gm$posterior, 1, which.max)
table(label)
default$outcome = label
relabel = rbind(train[train$outcome == 0,],default)
relabel
View(relabel)
train.qda <- qda(as.formula(outcome~.), data = relabel)
result = predict(train.qda, newdata = test)$posterior
predict_label_c1 = (1 - result[,1])>result[,1]
y_value = test$yc
acc = 1 - mean(predict_label_c1!=y_value)
y_value = test$outcome
acc = 1 - mean(predict_label_c1!=y_value)
acc
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
table(data$V9)
names(data)[9] = 'outcome'
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
table(data$outcome)
library(mixtools)
default = train[train$outcome == 1,]
gm<-mvnormalmixEM(default[,-9], k=3)
View(default)
gm<-mvnormalmixEM(default[,-9], k=2)
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
table(data$V9)
names(data)[9] = 'outcome'
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
table(data$outcome)
library(mixtools)
default = train[train$outcome == 1,]
gm<-mvnormalmixEM(default[,-9], k=2)
gm<-mvnormalmixEM(default[,-9], k=3)
gm<-mvnormalmixEM(default[,-9], k=3)
gm<-mvnormalmixEM(default[,-9], k=3)
gm<-mvnormalmixEM(default[,-9], k=2)
label = apply(gm$posterior, 1, which.max)
table(label)
setwd("~/Desktop/relabel in lda/data/ionosphere")
data = read.csv('ionosphere.csv')
View(data)
data = read.csv('ionosphere.csv',header = F)
View(data)
View(data)
data = data[,-c(1,2)]
data$V35 == 'g'
View(data)
table(data$V35)
data$V35 == 'b'
data$outcome = data$V35 == 'b'
View(data)
data = data[,-33]
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
table(data$outcome)
library(MASS)
train.lda <- qda(as.formula(outcome~.), data = train)
result = predict(train.lda, newdata = train)$posterior
predict_label_c1 = result[,2]>result[,1]
y_value = train$outcome
accracy = 1 - mean(predict_label_c1!=y_value)
accracy
result = predict(train.lda, newdata = test)$posterior
predict_label_c1 = result[,2]>result[,1]
y_value = test$outcome
accracy = 1 - mean(predict_label_c1!=y_value)
accracy
library(MASS)
train = synth.te
test = synth.tr
clusters <- kmeans(train[train$yc == 1,1:2], 2)
train[train$yc == 1,]$yc =  clusters$cluster
train$yc = as.factor(train$yc)
trainX <- train[,-3]
preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
library(caret)
preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
preProcValues
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 5,number = 10) #,classProbs=TRUE,summaryFunction = twoClassSummary)
library(parallel)
library(doMC)
registerDoMC(8)
knnFit <- train(yc ~ ., data = train, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:300)))
knnFit
plot(knnFit)
knnFit$results[knnFit$results$k == as.numeric(knnFit$bestTune),]$Accuracy
knnFit <- train(yc ~ ., data = train, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:300)))
knnFit
plot(knnFit)
knnFit$results[knnFit$results$k == as.numeric(knnFit$bestTune),]$Accuracy
knnPredict <- predict(knnFit,newdata = test,type="prob" )
grid <- expand.grid(xs=seq(-1.1,1.1,0.01), ys=seq(-0.2,1.2,0.01))
prob.grid <-  predict(knnFit, newdata = grid,type="prob")
z=matrix(prob.grid[,1], nrow=length(seq(-1.1,1.1,0.01)))
plot(train[,c(1:2)])
points(train[train$yc == 1,c(1:2)],col = 'red')
points(train[train$yc == 2,c(1:2)],col = 'blue')
contour(x=seq(-1.1,1.1,0.01), y=seq(-0.2,1.2,0.01), z = z, levels=0.5,
col="red", drawlabels=FALSE, lwd=4,add = TRUE)
predict_label_c1 <- c(predict(knnFit,newdata = train,type="prob" )[,1]>0.5)
y_value = c(train$yc == 0)
acc = 1 - mean(predict_label_c1!=y_value)
predict_label_c1 <- c(predict(knnFit,newdata = test,type="prob" )[,1]>0.5)
y_value = c(test$yc == 0)
acc = 1 - mean(predict_label_c1!=y_value)
library(mixtools)
acc
library(MASS)
train = synth.te
test = synth.tr
library(class)
prc_test_pred <- knn(train = train[,-3], test = test[,-3], cl = train$yc, k=64)
y_value = test$yc
accracy = 1 - mean(prc_test_pred!=y_value)
accracy
plot(train[,c(1:2)])
points(train[train$yc == 1,c(1:2)],col = 'red')
library(caret)
train$yc = as.factor(train$yc)
trainX <- train[,-3]
preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
preProcValues
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 5,number = 10) #,classProbs=TRUE,summaryFunction = twoClassSummary)
library(parallel)
library(doMC)
registerDoMC(8)
knnFit <- train(yc ~ ., data = train, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:300)))
knnFit
plot(knnFit)
knnFit$results[knnFit$results$k == as.numeric(knnFit$bestTune),]$Accuracy
grid <- expand.grid(xs=seq(-1.1,1.1,0.01), ys=seq(-0.2,1.2,0.01))
prob.grid <-  predict(knnFit, newdata = grid,type="prob")
z=matrix(prob.grid[,1], nrow=length(seq(-1.1,1.1,0.01)))
contour(x=seq(-1.1,1.1,0.01), y=seq(-0.2,1.2,0.01), z = z, levels=0.5,
col="red", drawlabels=FALSE, lwd=4,add = TRUE)
knnPredict <- predict(knnFit,newdata = test,type="prob" )
y_value = c(test$yc == 0)
acc = 1 - mean(c(knnPredict[,1]>0.5)!=y_value)
library(MASS)
acc
train_generate = function(x){
default = train[which(train$yc == 1),]
non_default = train[which(train$yc == 0),]
new.data = as.data.frame(rbind(non_default,default))
new.data = new.data[,c(1,2)]
colnames(new.data) = c("x1","x2")
c = rep("c2",nrow(default))
c[which(x == 1)] = "c3"
new.data$class = c(rep("c1",nrow(non_default)), c)
return(new.data)
}
clusters <- kmeans(train[train$yc == 1,1:2], 2)
sol = clusters$cluster
c1 = train[which(train$yc == 1),][which(sol == 1),]
c2 = train[which(train$yc == 1),][which(sol == 2),]
plot(train[,c(1:2)])
points(c1[c(1:2)],col = 'red')
points(c2[c(1:2)],col = 'blue')
chromosome = c(sol-1)
suggestion = chromosome
mldata = train_generate(chromosome)
View(mldata)
trainX <- train[,-3]
library(caret)
preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 5,number = 10) #,classProbs=TRUE,summaryFunction = twoClassSummary)
library(parallel)
library(doMC)
registerDoMC(8)
knnFit <- train(yc ~ ., data = train, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:50)))
result = predict(knnFit,newdata = test,type="prob" )
View(result)
knnFit
View(train)
knnFit <- train(yc ~ ., data = mldata, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:50)))
View(mldata)
knnFit <- train(class ~ ., data = mldata, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:50)))
knnFit
result = predict(knnFit,newdata = test,type="prob" )
View(mldata)
result = predict(knnFit,newdata = mldata,type="prob" )
View(result)
results = result[,2]+result[,3]
predict_label_c1 = c(result[,1]>results)
y_value = c(mldata$class == 'c1')
er = Error.rate(Fit = predict_label_c1, Y=y_value)
library(wSVM)
er = Error.rate(Fit = predict_label_c1, Y=y_value)
accracy
library(MASS)
train = synth.te
test = synth.tr
library(class)
prc_test_pred <- knn(train = train[,-3], test = test[,-3], cl = train$yc, k=64)
y_value = test$yc
accracy = 1 - mean(prc_test_pred!=y_value)
accracy
plot(train[,c(1:2)])
points(train[train$yc == 1,c(1:2)],col = 'red')
library(caret)
train$yc = as.factor(train$yc)
trainX <- train[,-3]
preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
preProcValues
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 5,number = 10) #,classProbs=TRUE,summaryFunction = twoClassSummary)
library(parallel)
library(doMC)
registerDoMC(8)
knnFit <- train(yc ~ ., data = train, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:100)))
knnFit
plot(knnFit)
knnFit$results[knnFit$results$k == as.numeric(knnFit$bestTune),]$Accuracy
grid <- expand.grid(xs=seq(-1.1,1.1,0.01), ys=seq(-0.2,1.2,0.01))
prob.grid <-  predict(knnFit, newdata = grid,type="prob")
z=matrix(prob.grid[,1], nrow=length(seq(-1.1,1.1,0.01)))
contour(x=seq(-1.1,1.1,0.01), y=seq(-0.2,1.2,0.01), z = z, levels=0.5,
col="red", drawlabels=FALSE, lwd=4,add = TRUE)
knnPredict <- predict(knnFit,newdata = test,type="prob" )
y_value = c(test$yc == 0)
acc = 1 - mean(c(knnPredict[,1]>0.5)!=y_value)
acc
contour(x=seq(-1.1,1.1,0.01), y=seq(-0.2,1.2,0.01), z = z, levels=0.5,
col="red", drawlabels=FALSE, lwd=4,add = TRUE)
library(MASS)
train = synth.te
test = synth.tr
clusters <- kmeans(train[train$yc == 1,1:2], 2)
train[train$yc == 1,]$yc =  clusters$cluster
library(MASS)
train = synth.te
test = synth.tr
clusters <- kmeans(train[train$yc == 1,1:2], 2)
train_generate = function(x){
default = train[which(train$yc == 1),]
non_default = train[which(train$yc == 0),]
new.data = as.data.frame(rbind(non_default,default))
new.data = new.data[,c(1,2)]
colnames(new.data) = c("x1","x2")
c = rep("c2",nrow(default))
c[which(x == 1)] = "c3"
new.data$class = c(rep("c1",nrow(non_default)), c)
return(new.data)
}
evalFunc <- function(chromosome){
if(sum(chromosome) == 0 | sum(chromosome) == length(chromosome)){
return(0)
} else {
mldata = train_generate(chromosome)
#f <- as.formula(paste("class ~ 1|", paste(names(mldata)[c(1,2)], collapse = " + ")))
#train$yc = as.factor(train$yc)
#trainX <- train[,-3]
#library(caret)
#preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
#preProcValues
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 5,number = 10) #,classProbs=TRUE,summaryFunction = twoClassSummary)
#library(doParallel)
library(parallel)
library(doMC)
registerDoMC(8)
knnFit <- train(class ~ ., data = mldata, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:50)))
result = predict(knnFit,newdata = mldata,type="prob" )
results = result[,2]+result[,3]
predict_label_c1 = c(result[,1]>results)
y_value = c(mldata$class == 'c1')
er = Error.rate(Fit = predict_label_c1, Y=y_value)
accracy = 1-er
return(accracy)
}
}
clusters <- kmeans(train[train$yc == 1,1:2], 2)
sol = clusters$cluster
c1 = train[which(train$yc == 1),][which(sol == 1),]
c2 = train[which(train$yc == 1),][which(sol == 2),]
plot(train[,c(1:2)])
points(c1[c(1:2)],col = 'red')
points(c2[c(1:2)],col = 'blue')
chromosome = c(sol-1)
suggestion = chromosome
pr = table(suggestion)/length(suggestion)
result <- t(sapply(1:99, function(x) {sample(c(0, 1), size = length(suggestion), replace = TRUE, prob = pr)}))
suggestion = rbind(suggestion, result)
library(wSVM)
evalFunc(suggestion[1,])
library(GA)
library(memoise)
GA = ga("binary", fitness = mfitness, nBits = nrow(train[which(train$yc == 1),]), popSize = 100, maxiter = 100, seed = 1, monitor = FALSE, parallel = 8, suggestions = suggestion)
mfitness <- memoise(evalFunc)
GA = ga("binary", fitness = mfitness, nBits = nrow(train[which(train$yc == 1),]), popSize = 100, maxiter = 100, seed = 1, monitor = FALSE, parallel = 8, suggestions = suggestion)
forget(mfitness)
summary(GA)
train[train$yc == 1,]$yc =  clusters$cluster
train$yc = as.factor(train$yc)
#trainX <- train[,-3]
library(caret)
preProcValues <- preProcess(x = trainX, method = c("center", "scale"))
preProcValues
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 5,number = 10) #,classProbs=TRUE,summaryFunction = twoClassSummary)
library(doParallel)
#
library(doMC)
registerDoMC(8)
knnFit <- train(yc ~ ., data = train, method = "knn", trControl = ctrl, preProcess = c("center","scale")
, tuneGrid = expand.grid(k = c(1:300)))
knnFit
plot(knnFit)
knnPredict <- predict(knnFit,newdata = test,type="prob" )
grid <- expand.grid(xs=seq(-1.1,1.1,0.01), ys=seq(-0.2,1.2,0.01))
prob.grid <-  predict(knnFit, newdata = grid,type="prob")
z=matrix(prob.grid[,1], nrow=length(seq(-1.1,1.1,0.01)))
contour(x=seq(-1.1,1.1,0.01), y=seq(-0.2,1.2,0.01), z = z, levels=0.5,
col="red", drawlabels=FALSE, lwd=4,add = TRUE)
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
data = read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",header = F)
table(data$V9)
names(data)[9] = 'outcome'
smp_size <- floor(0.75 * nrow(data))
set.seed(113)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
table(data$outcome)
library(MASS)
train.lda <- qda(as.formula(outcome~.), data = train)
result = predict(train.lda, newdata = train)$posterior
predict_label_c1 = result[,2]>result[,1]
y_value = train$outcome
accracy = 1 - mean(predict_label_c1!=y_value)
accracy
result = predict(train.lda, newdata = test)$posterior
predict_label_c1 = result[,2]>result[,1]
y_value = test$outcome
accracy = 1 - mean(predict_label_c1!=y_value)
accracy
library(mixtools)
default = train[train$outcome == 1,]
gm<-mvnormalmixEM(default[,-9], k=2)
label = apply(gm$posterior, 1, which.max)
table(label)
51.0360426*4135536-52*4242355
6010242-5592433
5592433-5135345
430000+6010242
6440242*434/8.6733
3927423*52
3454745*70/1.2924
187118655/(204225996+187118655)
6995/137.06
65/1.3142
70/1.2942
434/8.6733
214/4.2802
